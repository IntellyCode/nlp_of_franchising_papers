{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4780b952",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Before running the code, please install Tesseract on your laptop (https://github.com/UB-Mannheim/tesseract/wiki)\n",
    "\n",
    "The following code consists of 4 blocks:\n",
    "\n",
    "- Block 0 installs all the Python libraries that are needed to run the code. This part needs to be run only the very first time you use the code. __Once the libraries are installed, you do not need to reinstall them.__\n",
    "\n",
    "- Block 1 defines the user-specific inputs. Here you need to input the path to the folder containing the PDF files you want to analyze and the keywords (search terms) that define your topic of interest. __You need to run this block every time you use this code.__\n",
    "\n",
    "- Block 2 extracts the text from the PDF files. Once the text is extracted it will be stored as a .txt file. Accessing text from a .txt file is much faster than from a PDF file. __You need to run this part of code only when you are working on a new set of PDF files.__\n",
    "\n",
    "- Block 3 selects relevant papers using TF-IDF and runs LDA. __You must run this block every time you want to analyze (or re-analyze) a corpus of papers.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef42c9",
   "metadata": {},
   "source": [
    "# Block 0 - Before starting, install the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f1a35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#install necessary libraries\n",
    "#this step is required only the very first time you use this code\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} -c conda-forge poppler\n",
    "!{sys.executable} -m pip install --user pdf2image\n",
    "!conda install --yes --prefix {sys.prefix} -c conda-forge pytesseract\n",
    "!{sys.executable} -m pip install --user opencv-python\n",
    "!{sys.executable} -m pip install --user pyldavis\n",
    "!{sys.executable} -m pip install -U pip setuptools wheel\n",
    "!{sys.executable} -m pip install -U spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6a1cc",
   "metadata": {},
   "source": [
    "# Block 1 - Define user specific inputs (INPUT REQUIRED)\n",
    "\n",
    "Important: In the following code snippet, replace the path and search terms (keywords) with your own path to the folder containing PDF files of the papers and search keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk import SnowballStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#USER INPUTS\n",
    "#folder where all the papers PDF files are stored; \n",
    "#make sure there are no other files in the folder beyond the pdfs of the papers you are interested in \n",
    "path_to_folder='C:/Users/Desktop/Literature_review/' #INPUT HERE: path must be inputted with slashes,i.e. \"/\"; using  backslashes, i.e.\"\\\", will cause an error\n",
    "\n",
    "#list of words used to search for papers\n",
    "search_keywords=['innovation','innovations','new product','new products', 'culture','cultures'] #INPUT HERE\n",
    "\n",
    "\n",
    "#Stemming keywords - this step is important to identify relevant papers later\n",
    "stemmer = SnowballStemmer('english')\n",
    "keyword_stems=[]\n",
    "for keyword in search_keywords:\n",
    "    stems=[stemmer.stem(token) for token in keyword.split()]\n",
    "    for s in stems:\n",
    "        keyword_stems.append(s)\n",
    "\n",
    "print(set(keyword_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4ff2e",
   "metadata": {},
   "source": [
    "Stemming keywords allows us to reduce variance in the terms used to identify the concept you are interested in in the papers. These stems will later be used to distinguish relevant vs. irrelevant papers. For example, without stemming, \"innovation\", \"innovations\", \"innovative\", \"innovate\" and so on are to the computer completely different things. For a person, instead, they all communicate that the paper is discussing something related to \"innovation\". To make the computer capture these commonalities, we use stemming. \n",
    "\n",
    "It is necessary that you read this list of stems (see above) and decide which stems or combinations of stems, are more representative of what you are interested in. Hence, __it is now necessary to input the final list of stems you consider relevant.__ If you are interested in concepts that are represented by a combination of stems, they should be added to the list as \"stem1_stem2\", as for example \"new_product\" in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e12510",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_stems=['innov','cultur','new_product'] # #INPUT HERE: replace with the stems you are interested in \n",
    "papers_list = []\n",
    "for file in os.listdir(path_to_folder):\n",
    "    if file.endswith('.pdf'):\n",
    "        papers_list.append(file)\n",
    "    #gets the list of files in the folder\n",
    "papers_ann = pd.DataFrame (papers_list, columns = ['File Name'])\n",
    "\n",
    "#generate folders where outputs will be stored\n",
    "folders_names=[\"Pages_to_images\",\"Results\",\"Txt_files\"]\n",
    "for folder in folders_names:\n",
    "    if os.path.exists(path_to_folder+folder) == False:\n",
    "        os.mkdir(os.path.join(path_to_folder, folder)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f7d2a",
   "metadata": {},
   "source": [
    "# Block 2 - Extracting text from PDF files\n",
    "\n",
    "The first step in analyzing papers is extracting text from PDFs. The best way to retrieve text from PDFs is to first convert each page in an image and only then to retrieve text from each image using optical character recognition (OCR). Although this is more computationally intensive, OCR libraries produce, in our experience, more accurate results than PDF reading libraries. \n",
    "\n",
    "The following code snippet converts each page of the PDFs to an image. The image is saved in the same location as the PDF files and named after the PDF file name and page number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    for index, image in enumerate(images):\n",
    "        image.save(f'{pdf_path[0: -4]}-{index}.png')\n",
    "        \n",
    "\n",
    "w = path_to_folder[:2] + \"\\\\\"\n",
    "\n",
    "for r,d,f in os.walk(w):\n",
    "    for files in f:\n",
    "         if files == \"tesseract.exe\":\n",
    "              tess_path=os.path.join(r,files)\n",
    "              break\n",
    "\n",
    "for p in range(len(papers_ann)):\n",
    "    path = path_to_folder + papers_ann['File Name'][p]\n",
    "    print(papers_ann['File Name'][p])\n",
    "    convert_pdf_to_images(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa43c3a",
   "metadata": {},
   "source": [
    "The following code snippet extracts text from each of the previously created images and generates two .txt files for each paper. The first one is named after the original PDF file and contains the entire main body of the paper. The second one is named as \"[original PDF file name]-ref.txt\" and contains the list of references in the paper. \n",
    "\n",
    "If the paper’s main body does not start on the first page, the code will include the cover page information (e.g., title, author(s), source, published by, stable URL, etc.) in the main body txt file. Notably, 75% of the papers in our sample start on the first page. Hence, this approach substantially reduces the amount of manual work needed, while adding  only a negligible amount of useless text to a small number of papers.\n",
    "\n",
    "We separate the reference list from the main text because reference lists might affect word counts. For example, in Hurley and Hult (1998) “innovation” is mentioned 156 times including the reference list and 127 times excluding it. \n",
    "Reference lists are saved because they can be helpful in identifying other potentially relevant papers. If you are certain that all the papers have the same citation style, these reference files could be used to generate useful stats. Given that each paper in the reference  list file appears in a new line, it is very easy to import references into Excel and then work on them as a list. We did not focus on providing this functionality within this code as there are already online tools that analyze citation networks (see e.g., https://www.connectedpapers.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cc940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import re\n",
    "#import os.path\n",
    "import pandas as pd\n",
    "\n",
    "for p in range(len(papers_ann)):\n",
    "    path=path_to_folder+ papers_ann['File Name'][p]\n",
    "    print(papers_ann['File Name'][p])\n",
    "    pagestxt = []\n",
    "    i=0\n",
    "    while os.path.exists(f'{path[0: -4]}-{str(i)}.png') == True:\n",
    "        img_path=f'{path[0: -4]}-{str(i)}.png'\n",
    "        #print(img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "        kernel = np.ones((2, 1), np.uint8)\n",
    "        img = cv2.erode(img, kernel, iterations=1)\n",
    "        img = cv2.dilate(img, kernel, iterations=1)\n",
    "        pytesseract.pytesseract.tesseract_cmd = tess_path\n",
    "        out_below = pytesseract.image_to_string(img)\n",
    "        pagestxt.append(out_below)\n",
    "        #print(pagestxt)\n",
    "        i+=1\n",
    "    len_splits=[]\n",
    "    for z in range (0,len(pagestxt)):\n",
    "        a = re.split(r'R[eE][fF][eE][rR][eE][nN][cC][eE][sS]',pagestxt[-abs(z)])\n",
    "        len_splits.append(len(a))\n",
    "        if len(a)!=1:\n",
    "            papertext=pagestxt[:-abs(z)]+[a[0]]\n",
    "            txt_path = f'{path[0: -4]}.txt'\n",
    "            file2 = open(txt_path,\"w+\")\n",
    "            file2.write(\" \".join(papertext))\n",
    "            file2.close()\n",
    "\n",
    "            #The following 4 lines generate the reference file, if you don't want to store references, you can mute them.\n",
    "            reftxt=[a[1]]+pagestxt[-abs(z-1):] \n",
    "            ref_path = f'{path[0: -4]}-ref.txt'\n",
    "            file3 = open(ref_path,\"w+\")\n",
    "            file3.write(\" \".join(reftxt)) \n",
    "            file3.close()\n",
    "        elif z==len(pagestxt)-1 and set(len_splits)=={1}:\n",
    "             papertext=pagestxt\n",
    "             txt_path = f'{path[0: -4]}.txt'\n",
    "             file4 = open(txt_path,\"w+\")\n",
    "             file4.write(\" \".join(papertext))\n",
    "             file4.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moves images and txts to dedicated folders to declutter the original folder\n",
    "import shutil\n",
    "source = os.listdir(path_to_folder)\n",
    "destination = path_to_folder+'Pages_to_images'\n",
    "destination1 = path_to_folder+\"Txt_files\"\n",
    "for files in source:\n",
    "    if files.endswith('.png'):\n",
    "        shutil.move(os.path.join(path_to_folder,files), os.path.join(destination,files))\n",
    "    elif files.endswith('.txt'):\n",
    "        shutil.move(os.path.join(path_to_folder,files), os.path.join(destination1,files))\n",
    "        \n",
    "papers_ann.to_excel(path_to_folder+\"Results/\"+'Papers_list.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32b398",
   "metadata": {},
   "source": [
    "# Block 3 \n",
    "## Step 3.1 - Compute TF-IDF values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54054810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk import SnowballStemmer\n",
    "#import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk import BigramCollocationFinder, BigramAssocMeasures,pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer,snowball\n",
    "\n",
    "#COLOCATIONS\n",
    "def join_collocations(element, known_collocations):\n",
    "    result = []\n",
    "    is_collocation = False\n",
    "    current_chain = []\n",
    "    for i, w in enumerate(element):\n",
    "        if i < len(element) - 1 and (w, element[i + 1]) in known_collocations:\n",
    "            if current_chain == []:\n",
    "                current_chain = [w, element[i + 1]]\n",
    "            else:\n",
    "                current_chain.append(element[i+1])\n",
    "            is_collocation = True\n",
    "        else:\n",
    "            if is_collocation:\n",
    "                result.append('_'.join(current_chain))\n",
    "                current_chain = []\n",
    "            else:\n",
    "                result.append(w)\n",
    "            is_collocation = False\n",
    "    return result\n",
    "\n",
    "def apply_collocations(sentence, set_colloc):\n",
    "    for b1,b2 in set_colloc:\n",
    "        sentence = sentence.replace(\"%s %s\" % (b1 ,b2), \"%s_%s\" % (b1 ,b2))\n",
    "    return sentence\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "documents=[]\n",
    "\n",
    "for i in range(len(papers_ann)):\n",
    "    path = path_to_folder +\"Txt_files/\"+ papers_ann['File Name'][i]\n",
    "    print(papers_ann['File Name'][i])\n",
    "    txt_path = f'{path[0: -4]}.txt'\n",
    "    a = open(txt_path,'r')\n",
    "    documents.append(a.read())\n",
    "  \n",
    "exclist = string.punctuation + string.digits\n",
    "stemmer = SnowballStemmer('english')\n",
    "for i in range(len(documents)):\n",
    "    # remove punctuations and digits from oldtext\n",
    "    table_ = str.maketrans('', '', exclist)\n",
    "    documents[i] = documents[i].translate(table_)\n",
    "    documents[i]=remove_stopwords(documents[i].lower())\n",
    "    documents[i]=\" \".join([token.lemma_ for token in nlp(documents[i])])\n",
    "    documents[i] = \" \".join([stemmer.stem(token) for token in documents[i].split()])\n",
    "    words=[token for token in documents[i].split()]\n",
    "\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    bgm = BigramAssocMeasures()\n",
    "    collocations = [b for b, f in finder.score_ngrams(bgm.mi_like) if f > 1]\n",
    "\n",
    "    \n",
    "    col = pd.DataFrame({'col': collocations})\n",
    "    documents[i] = apply_collocations(documents[i], set_colloc=collocations)\n",
    "\n",
    "vect = TfidfVectorizer(analyzer='word', sublinear_tf=True)\n",
    "tfidf_matrix = vect.fit_transform(documents)\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
    "#if you are using Sklearn >= 1.0.x, you must replace get_feature_names() with get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cb375",
   "metadata": {},
   "source": [
    "## Step 3.2 - Identify relevant papers according to TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15506202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for k in keywords_stems:\n",
    "    papers_ann['tfidf_'+ k]=np.nan\n",
    "    papers_ann['relevant_'+ k]=np.nan\n",
    "    for i in range(len(papers_ann)):\n",
    "        papers_ann['tfidf_'+ k][i]=df[k][i]\n",
    "        if papers_ann['tfidf_'+k][i]>=papers_ann['tfidf_'+k].mean():\n",
    "            papers_ann['relevant_'+k][i]='Y'\n",
    "        else:\n",
    "            papers_ann['relevant_'+k][i]='N'\n",
    "\n",
    "res = list(combinations(keywords_stems, 2))\n",
    "for r in res:\n",
    "    papers_ann['relevant_'+str(r)]=np.nan\n",
    "    for i in range(len(papers_ann)):\n",
    "        if papers_ann['tfidf_'+r[0]][i]>=papers_ann['tfidf_'+r[0]].mean() or papers_ann['tfidf_'+r[1]][i]>=papers_ann['tfidf_'+r[1]].mean():\n",
    "            papers_ann['relevant_'+str(r)][i]='Y'\n",
    "        else:\n",
    "            papers_ann['relevant_'+str(r)][i]='N'\n",
    "\n",
    "\n",
    "papers_ann.to_excel(path_to_folder +'Results/'+'tfidf-results.xlsx')\n",
    "papers_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d43aa",
   "metadata": {},
   "source": [
    "## Step 3.3 -  Topic Modeling using LDA (INPUT REQUIRED)\n",
    "\n",
    "In this stage you need to select which criterion (e.g., which tf-idf threshold) you find more informative for distinguishing relevant vs. irrelevant papers. To validate our approach, we first manually annotated papers and then compared tf-idf results to manual annotation, finding that the combination (\"innov\", \"new_product\") was the most accurate. Hence, based on our experience, we suggest the following when determining which criteria to focus on: \n",
    "1) combinations of stems are likely to provide more comprehensive lists; \n",
    "2) focus on combinations of synonyms of the concept; and \n",
    "3) avoid considering all your stems together.\n",
    "\n",
    "If you do not know which criteria to choose, you can run the code multiple times, using  different criteria, and then check which criteria produces the results you find most informative for your research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn #if you are using pyLDAvis >= 3.4.0, you must replace pyLDAvis.sklearn with pyLDAvis.lda_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Data\n",
    "\n",
    "listofpapers=[]\n",
    "\n",
    "papers_rel=papers_ann[papers_ann[\"relevant_('innov', 'new_product')\"]==\"Y\"] # INPUT HERE: replace the column name with the criterion you think is most relevant\n",
    "\n",
    "\n",
    "papers_rel.reset_index(inplace=True)\n",
    "print(len(papers_rel.index))\n",
    "papers_rel.to_excel(path_to_folder +'Results/'+'paper_rel.xlsx')\n",
    "papers_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(papers_rel)):\n",
    "    path = path_to_folder +\"Txt_files/\"+ papers_rel['File Name'][i]\n",
    "    txt_path = f'{path[0: -4]}.txt'\n",
    "    a = open(txt_path,'r')\n",
    "    listofpapers.append(a.read())\n",
    "print(len(listofpapers))\n",
    "\n",
    "# Convert to list\n",
    "data = listofpapers\n",
    "data = [sent.replace('-\\n', '') for sent in data]\n",
    "data = [sent.replace('\\n', ' ') for sent in data]\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] and len(token.lemma_)>2 else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             min_df=10,# minimum reqd occurences of a word\n",
    "                             #max_df=2000,\n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "voc = vectorizer.vocabulary_\n",
    "print(voc)\n",
    "\n",
    "\n",
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes\n",
    "\n",
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [2,3, 5, 7, 10, 12, 15,20], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, n_jobs=-1,cv=10)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "\n",
    "# How to see the best topic model and its parameters?\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "\n",
    "\n",
    "# Compare LDA Model Performance Scores\n",
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [2,3, 5, 7, 10, 12, 15,20]\n",
    "grids=pd.DataFrame(model.cv_results_)\n",
    "\n",
    "log_likelyhoods_5 = list(grids[grids['param_learning_decay']==0.5]['mean_test_score'])\n",
    "log_likelyhoods_7 = list(grids[grids['param_learning_decay']==0.7]['mean_test_score'])\n",
    "log_likelyhoods_9 = list(grids[grids['param_learning_decay']==0.9]['mean_test_score'])\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()\n",
    "\n",
    "# How to see the dominant topic in each document?\n",
    "\n",
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [ str(i) for i in range(len(data))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "\n",
    "df_document_topic.to_excel(path_to_folder+'Results/'+\"main_topics-dominant_topics.xlsx\")\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics\n",
    "\n",
    "#Review topics distribution across documents\n",
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution\n",
    "\n",
    "#Visualize the LDA model\n",
    "import pyLDAvis.sklearn#if you are using pyLDAvis >= 3.4.0, you must replace pyLDAvis.sklearn with pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer,mds='tsne',sort_topics=False)#if you are using pyLDAvis >= 3.4.0, you must replace pyLDAvis.sklearn with pyLDAvis.lda_model\n",
    "pyLDAvis.save_html(panel, path_to_folder+'Results/'+'lda.html')\n",
    "panel\n",
    "\n",
    "all_topics = {}\n",
    "num_terms = 10 # Adjust number of words to represent each topic\n",
    "lambd = 0.6 # Adjust this accordingly based on tuning above\n",
    "for i in range(1,3): #Adjust this to reflect number of topics chosen for final LDA model\n",
    "    topic = panel.topic_info[panel.topic_info.Category == 'Topic'+str(i)].copy()\n",
    "    topic['relevance'] = topic['loglift']*(1-lambd)+topic['logprob']*lambd\n",
    "    all_topics['Topic '+str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values\n",
    "    \n",
    "pd.DataFrame(all_topics).T\n",
    "\n",
    "# Topic’s keywords\n",
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()#if you are using Sklearn >= 1.0.x, you must replace get_feature_names() with get_feature_names_out()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()\n",
    "\n",
    "# top 15 keywords each topic\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())#if you are using Sklearn >= 1.0.x, you must replace get_feature_names() with get_feature_names_out()\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=10)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords.to_excel(path_to_folder+'Results/'+\"top10words.xlsx\", \"Main Topics\")\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a08c8",
   "metadata": {},
   "source": [
    "## Step 3.4 - Subtopics for loop (optional)\n",
    "You now have the topics for your entire sample stored in the Results folder. You can stop your analysis here, if you find these results informative enough.\n",
    "Alternatively, the following code snippet allows you to identify subtopics within the sets of papers allocated to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d78801",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list=df_document_topic['dominant_topic'].unique() #get the list of topics identified in the sample\n",
    "for tpc in topics_list:\n",
    "    list_index_topic0 = df_document_topic[df_document_topic['dominant_topic']==tpc]\n",
    "    list_index_topic0.reset_index(inplace=True)\n",
    "    list_index_topic0.head()\n",
    "    docs_in_topic0=list(list_index_topic0['index'])\n",
    "    print(docs_in_topic0)\n",
    "    print(len(docs_in_topic0))\n",
    "    \n",
    "    \n",
    "    data_topic1=[]\n",
    "    for i in docs_in_topic0:\n",
    "        data_topic1.append(data_lemmatized[int(i)])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word',\n",
    "                                 min_df=1,                        # minimum reqd occurences of a word \n",
    "                                 stop_words='english',             # remove stop words\n",
    "                                 lowercase=True,                   # convert all words to lowercase\n",
    "                                 token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                                 # max_features=50000,             # max number of uniq words\n",
    "                                )\n",
    "\n",
    "    data_vectorized = vectorizer.fit_transform(data_topic1)\n",
    "    \n",
    "    \n",
    "    data_dense = data_vectorized.todense()\n",
    "\n",
    "    # Compute Sparsicity = Percentage of Non-Zero cells\n",
    "    print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "    # Build LDA Model\n",
    "    lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                          max_iter=10,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=128,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "    print(lda_model)  # Model attributes\n",
    "    \n",
    "    # Log Likelyhood: Higher the better\n",
    "    print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "    # See model parameters\n",
    "    pprint(lda_model.get_params())\n",
    "    \n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [2,3, 5, 7, 10, 12, 15,20], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params, n_jobs=-1,cv=10)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(data_vectorized)\n",
    "    \n",
    "    #How to see the best topic model and its parameters?\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "    # Log Likelihood Score\n",
    "    print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "    # Perplexity\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "    \n",
    "    # Compare LDA Model Performance Scores\n",
    "    # Get Log Likelyhoods from Grid Search Output\n",
    "    n_topics = [2, 3, 5, 7, 10, 12, 15,20]\n",
    "    grids=pd.DataFrame(model.cv_results_)\n",
    "    log_likelyhoods_5 = list(grids[grids['param_learning_decay']==0.5]['mean_test_score'])\n",
    "    log_likelyhoods_7 = list(grids[grids['param_learning_decay']==0.7]['mean_test_score'])\n",
    "    log_likelyhoods_9 = list(grids[grids['param_learning_decay']==0.9]['mean_test_score'])\n",
    "    # Show graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "    plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "    plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "    plt.title(\"Choosing Optimal LDA Model\")\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Log Likelyhood Scores\")\n",
    "    plt.legend(title='Learning decay', loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    # How to see the dominant topic in each document?\n",
    "    # Create Document - Topic Matrix\n",
    "    lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "    # index names\n",
    "    #docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "    docnames = [ docs_in_topic0[i] for i in range(len(data_topic1))]\n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic1 = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic1 = np.argmax(df_document_topic1.values, axis=1)\n",
    "    df_document_topic1['dominant_topic'] = dominant_topic1\n",
    "\n",
    "    # Styling\n",
    "    def color_green(val):\n",
    "        color = 'green' if val > .1 else 'black'\n",
    "        return 'color: {col}'.format(col=color)\n",
    "\n",
    "    def make_bold(val):\n",
    "        weight = 700 if val > .1 else 400\n",
    "        return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "    # Apply Style\n",
    "    df_document_topic1.to_excel(path_to_folder+'Results/'+\"dominant_topics-subtopic\"+str(tpc)+\".xlsx\")\n",
    "    df_document_topics1 = df_document_topic1.head(20).style.applymap(color_green).applymap(make_bold)\n",
    "    df_document_topics1\n",
    "    \n",
    "    # Review topics distribution across documents\n",
    "    df_topic_distribution1 = df_document_topic1['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "    df_topic_distribution1.columns = ['Topic Num', 'Num Documents']\n",
    "    df_topic_distribution1\n",
    "    \n",
    "    # How to visualize the LDA model with pyLDAvis?\n",
    "    import pyLDAvis.sklearn#if you are using pyLDAvis >= 3.4.0, you must replace pyLDAvis.sklearn with pyLDAvis.lda_model\n",
    "    pyLDAvis.enable_notebook()\n",
    "    panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer,mds='tsne',sort_topics=False)#if you are using pyLDAvis >= 3.4.0, you must replace pyLDAvis.sklearn with pyLDAvis.lda_model\n",
    "    pyLDAvis.save_html(panel, path_to_folder+'Results/'+'subtopics_within_'+str(tpc)+'.html')\n",
    "    panel\n",
    "    \n",
    "    \n",
    "    all_topics = {}\n",
    "    num_terms = 10 # Adjust number of words to represent each topic\n",
    "    lambd = 0.6 # Adjust this accordingly based on tuning above\n",
    "    for i in range(1,3): #Adjust this to reflect number of topics chosen for final LDA model\n",
    "        topic = panel.topic_info[panel.topic_info.Category == 'Topic'+str(i)].copy()\n",
    "        topic['relevance'] = topic['loglift']*(1-lambd)+topic['logprob']*lambd\n",
    "        all_topics['Topic '+str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values\n",
    "        \n",
    "    pd.DataFrame(all_topics).T\n",
    "    \n",
    "    \n",
    "    # Topic’s keywords\n",
    "    # Topic-Keyword Matrix\n",
    "    df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "    # Assign Column and Index\n",
    "    df_topic_keywords.columns = vectorizer.get_feature_names()#if you are using Sklearn >= 1.0.x, you must replace get_feature_names() with get_feature_names_out()\n",
    "    df_topic_keywords.index = topicnames\n",
    "\n",
    "    # View\n",
    "    df_topic_keywords.head()\n",
    "    \n",
    "    \n",
    "    # top 15 keywords each topic\n",
    "    # Show top n keywords for each topic\n",
    "    def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "        keywords = np.array(vectorizer.get_feature_names())#if you are using Sklearn >= 1.0.x, you must replace get_feature_names() with get_feature_names_out()\n",
    "        topic_keywords = []\n",
    "        for topic_weights in lda_model.components_:\n",
    "            top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "            topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "        return topic_keywords\n",
    "\n",
    "    topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=10)        \n",
    "\n",
    "    # Topic - Keywords Dataframe\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    #print(topic)\n",
    "    df_topic_keywords.to_excel(path_to_folder+'Results/'+\"top10words.xlsx\", \"Subtopics_\"+str(tpc))\n",
    "    #df_topic_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
